# app/agents/multimodal_agent.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Literal, BinaryIO
from datetime import datetime, timezone
from loguru import logger
from contextvars import ContextVar
import json
import io
import base64

from langchain.tools import tool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from app.integrations.gigachat_client import gigachat_client
from app.integrations import salutespeech_service
from app.integrations.minio_service import MinioService
from app.integrations import minio_service as MinioServiceDep
from app.config import settings


@dataclass
class MultimodalContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è Multimodal Agent"""
    user_id: int
    uploaded_files: List[Dict[str, Any]]


_multimodal_context: ContextVar[Optional[MultimodalContext]] = ContextVar(
    '_multimodal_context',
    default=None
)

_minio_service: ContextVar[Optional[MinioService]] = ContextVar('_minio_service', default=None)



def _get_context() -> MultimodalContext:
    """Get the current context from ContextVar"""
    ctx = _multimodal_context.get()
    if ctx is None:
        raise RuntimeError("Multimodal context not set.")
    return ctx


def _get_minio_service() -> MinioService:
    service = _minio_service.get()
    if service is None:
        raise RuntimeError("Minio service not set.")
    return service


async def _get_file_from_ref(file_ref: Optional[str]) -> tuple[BinaryIO, str]:
    """
    Returns: (file_object(BytesIO), filename, object_name)
    """
    ctx = _get_context()
    minio_service = _get_minio_service()

    if not file_ref:
        if not ctx.uploaded_files:
            raise ValueError("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤. –£–∫–∞–∂–∏ file_ref –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª.")
        file_info = ctx.uploaded_files[0]
        object_name = file_info.get("object_name") or file_info.get("file_id")
        filename = file_info.get("filename", "unknown")
    else:
        file_info = next(
            (f for f in ctx.uploaded_files
             if f.get("object_name") == file_ref or f.get("file_id") == file_ref),
            None
        )
        object_name = file_ref
        filename = file_info.get("filename", "unknown") if file_info else "unknown"

    if not object_name:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å object_name —Ñ–∞–π–ª–∞")

    file_object = await minio_service.download_file(object_name)
    if file_object is None:
        raise ValueError(f"–§–∞–π–ª {object_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
    file_object.seek(0)

    size = len(file_object.getbuffer())
    logger.info(f"Loaded file: {filename} ({size} bytes), object_name={object_name}")

    return file_object, filename


# ============================================================================
# TOOLS
# ============================================================================

@tool
async def analyze_image(
    file_ref: Optional[str] = None,
    prompt: str = "–û–ø–∏—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–∞ –ø–∏—Ç–æ–º—Ü–∞.",
    temperature: float = 0.2,
) -> str:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ GigaChat Vision.
    
    –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è:
    - –ê–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ç–æ –ø–∏—Ç–æ–º—Ü–∞ (–≤–Ω–µ—à–Ω–∏–π –≤–∏–¥, —Å–æ—Å—Ç–æ—è–Ω–∏–µ)
    - –û—Ü–µ–Ω–∫–∏ —Å–∏–º–ø—Ç–æ–º–æ–≤ –ø–æ —Ñ–æ—Ç–æ (—Å—ã–ø—å, —Ä–∞–Ω—ã, –∏–∑–º–µ–Ω–µ–Ω–∏—è)
    - –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥—ã
    - –ê–Ω–∞–ª–∏–∑–∞ —É—Å–ª–æ–≤–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
    
    Args:
        file_ref: –°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª (object_name). –ï—Å–ª–∏ None - –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω—É–∂–Ω–æ –æ–ø–∏—Å–∞—Ç—å/–Ω–∞–π—Ç–∏)
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-1.0)
    
    Returns:
        JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞:
        {
          "analyzed_at": ISO8601,
          "filename": str,
          "prompt": str,
          "analysis": str (—Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç GigaChat Vision),
          "file_ref": str
        }
    """
    try:
        ctx = _get_context()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª
        file_object, filename = await _get_file_from_ref(file_ref)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GigaChat Vision
        analysis = await gigachat_client.vision_analysis(
            file=file_object,
            filename=filename,
            prompt=prompt,
            temperature=temperature
        )
        
        result = {
            "analyzed_at": datetime.now(timezone.utc).isoformat(),
            "filename": filename,
            "prompt": prompt,
            "analysis": analysis,
            "file_ref": file_ref or "auto_selected"
        }
        
        logger.info(f"Image analyzed: {filename}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"Failed to analyze image: {e}")
        return json.dumps({
            "error": str(e),
            "filename": filename if 'filename' in locals() else "unknown"
        }, ensure_ascii=False)


@tool
async def ocr_image(
    file_ref: Optional[str] = None,
    mode: Literal["plain", "structured"] = "structured",
) -> str:
    """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (OCR).
    
    –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è:
    - –ß—Ç–µ–Ω–∏—è —ç—Ç–∏–∫–µ—Ç–æ–∫ –∫–æ—Ä–º–æ–≤
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–ø—Ä–∞–≤–æ–∫
    - –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—Ü–µ–ø—Ç–æ–≤
    - –ß—Ç–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–æ–≤
    
    Args:
        file_ref: –°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª. –ï—Å–ª–∏ None - –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π
        mode: –†–µ–∂–∏–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
              - "plain": –ø—Ä–æ—Å—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç
              - "structured": –ø—ã—Ç–∞–µ—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏)
    
    Returns:
        JSON —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º:
        {
          "analyzed_at": ISO8601,
          "filename": str,
          "mode": str,
          "text": str (—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç),
          "structured_data": dict|null (–µ—Å–ª–∏ mode="structured"),
          "file_ref": str
        }
    """
    try:
        ctx = _get_context()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª
        file_object, filename = await _get_file_from_ref(file_ref)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        if mode == "plain":
            prompt = "–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
        else:  # structured
            prompt = """–ò–∑–≤–ª–µ–∫–∏ —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –µ–≥–æ.
            
–ï—Å–ª–∏ —ç—Ç–æ —ç—Ç–∏–∫–µ—Ç–∫–∞ –∫–æ—Ä–º–∞:
- –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å
- –°–æ—Å—Ç–∞–≤ (—Å–ø–∏—Å–æ–∫ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤)
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–±–µ–ª–∫–∏, –∂–∏—Ä—ã, –∫–ª–µ—Ç—á–∞—Ç–∫–∞)
- –ö–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å
- –î–∞—Ç–∞ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è/—Å—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏

–ï—Å–ª–∏ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç:
- –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –î–∞—Ç–∞
- –î–∏–∞–≥–Ω–æ–∑/–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

–í–µ—Ä–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–ª—è–º–∏."""
        
        # OCR —á–µ—Ä–µ–∑ GigaChat Vision
        ocr_result = await gigachat_client.vision_analysis(
            file=file_object,
            filename=filename,
            prompt=prompt,
            temperature=0.1  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        )
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –µ—Å–ª–∏ structured mode
        structured_data = None
        if mode == "structured":
            try:
                # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                import re
                json_match = re.search(r'\{.*\}', ocr_result, re.DOTALL)
                if json_match:
                    structured_data = json.loads(json_match.group(0))
            except:
                logger.warning("Failed to parse structured OCR result as JSON")
        
        result = {
            "analyzed_at": datetime.now(timezone.utc).isoformat(),
            "filename": filename,
            "mode": mode,
            "text": ocr_result,
            "structured_data": structured_data,
            "file_ref": file_ref or "auto_selected"
        }
        
        logger.info(f"OCR completed: {filename}, mode={mode}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"Failed to OCR image: {e}")
        return json.dumps({
            "error": str(e),
            "filename": filename if 'filename' in locals() else "unknown"
        }, ensure_ascii=False)


@tool
async def transcribe_audio(
    file_ref: Optional[str] = None,
    audio_format_hint: str = "audio/x-pcm;bit=16;rate=16000",
) -> str:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ SaluteSpeech STT.
    
    –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è:
    - –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤–∏–¥–µ–æ (–ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ)
    - –°–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∞
    
    Args:
        file_ref: –°—Å—ã–ª–∫–∞ –Ω–∞ –∞—É–¥–∏–æ —Ñ–∞–π–ª. –ï—Å–ª–∏ None - –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π
        audio_format_hint: –§–æ—Ä–º–∞—Ç –∞—É–¥–∏–æ (–¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
                          –ü—Ä–∏–º–µ—Ä—ã: "audio/x-pcm;bit=16;rate=16000"
                                   "audio/wav"
    
    Returns:
        JSON —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π:
        {
          "transcribed_at": ISO8601,
          "filename": str,
          "audio_format": str,
          "text": str (—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç),
          "duration_seconds": float|null,
          "file_ref": str
        }
    """
    try:
        ctx = _get_context()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª
        file_object, filename = await _get_file_from_ref(file_ref)
        audio_data = file_object.read()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–¥–∏–æ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞
        # –ü–∞—Ä—Å–∏–º audio/x-pcm;bit=16;rate=16000
        sample_rate = 16000
        bit_depth = 16
        
        if "rate=" in audio_format_hint:
            import re
            rate_match = re.search(r'rate=(\d+)', audio_format_hint)
            if rate_match:
                sample_rate = int(rate_match.group(1))
        
        if "bit=" in audio_format_hint:
            import re
            bit_match = re.search(r'bit=(\d+)', audio_format_hint)
            if bit_match:
                bit_depth = int(bit_match.group(1))
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ SaluteSpeech
        transcribed_text = await salutespeech_service.speech_to_text(
            audio_data=audio_data,
            sample_rate=sample_rate,
            bit_depth=bit_depth
        )
        
        # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        duration = None
        try:
            # –î–ª—è PCM: duration = bytes / (sample_rate * channels * bytes_per_sample)
            bytes_per_sample = bit_depth // 8
            channels = 1  # –ú–æ–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            duration = len(audio_data) / (sample_rate * channels * bytes_per_sample)
        except:
            pass
        
        result = {
            "transcribed_at": datetime.now(timezone.utc).isoformat(),
            "filename": filename,
            "audio_format": audio_format_hint,
            "text": transcribed_text,
            "duration_seconds": round(duration, 2) if duration else None,
            "file_ref": file_ref or "auto_selected"
        }
        
        logger.info(f"Audio transcribed: {filename}, length={len(transcribed_text)} chars")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"Failed to transcribe audio: {e}")
        return json.dumps({
            "error": str(e),
            "filename": filename if 'filename' in locals() else "unknown"
        }, ensure_ascii=False)


@tool
async def analyze_video(
    file_ref: Optional[str] = None,
    prompt: str = "–û–ø–∏—à–∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ/—Å–∏–º–ø—Ç–æ–º—ã –Ω–∞ –≤–∏–¥–µ–æ. –û—Ç–¥–µ–ª—å–Ω–æ: —á—Ç–æ –Ω–∞—Å—Ç–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –∏ –∫–∞–∫–∏–µ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏.",
    frame_count: int = 5,
    transcribe: bool = False,
) -> str:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ + –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Vision).
    
    –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è:
    - –ê–Ω–∞–ª–∏–∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–∏—Ç–æ–º—Ü–∞ –Ω–∞ –≤–∏–¥–µ–æ
    - –û—Ü–µ–Ω–∫–∏ —Å–∏–º–ø—Ç–æ–º–æ–≤ –≤ –¥–∏–Ω–∞–º–∏–∫–µ
    - –ê–Ω–∞–ª–∏–∑–∞ –ø–æ—Ö–æ–¥–∫–∏, –¥–≤–∏–∂–µ–Ω–∏–π
    - –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∏—Ç–æ–º—Ü–∞
    
    Args:
        file_ref: –°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ —Ñ–∞–π–ª. –ï—Å–ª–∏ None - –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π
        prompt: –ß—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –≤–∏–¥–µ–æ
        frame_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
        transcribe: –ò–∑–≤–ª–µ—á—å –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ
    
    Returns:
        JSON —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–∏–¥–µ–æ:
        {
          "analyzed_at": ISO8601,
          "filename": str,
          "prompt": str,
          "frame_count": int,
          "video_analysis": str (–∞–Ω–∞–ª–∏–∑ –∫–∞–¥—Ä–æ–≤),
          "audio_transcription": str|null (–µ—Å–ª–∏ transcribe=True),
          "frames_analyzed": [
            {"frame_number": int, "timestamp_sec": float}
          ],
          "file_ref": str
        }
    """
    try:
        ctx = _get_context()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª
        file_object, filename = await _get_file_from_ref(file_ref)
        video_data = file_object.read()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        import tempfile
        import cv2
        
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_video:
            temp_video.write(video_data)
            temp_video_path = temp_video.name
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã —á–µ—Ä–µ–∑ OpenCV
            cap = cv2.VideoCapture(temp_video_path)
            
            if not cap.isOpened():
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ —Ñ–∞–π–ª")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ –æ –≤–∏–¥–µ–æ
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ –∫–∞–¥—Ä—ã –∏–∑–≤–ª–µ–∫–∞—Ç—å (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ –≤–∏–¥–µ–æ)
            frame_count = max(1, frame_count)
            if total_frames <= 0:
                frame_indices = [0]
            else:
                step = max(1, total_frames // frame_count)
                frame_indices = [min(total_frames - 1, i * step) for i in range(frame_count)]
                frame_indices = sorted(set(frame_indices))            
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã
            extracted_frames = []
            frames_data = []
            
            for idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                
                if ret:
                    _, buffer = cv2.imencode('.jpg', frame)
                    frame_bytes = io.BytesIO(buffer.tobytes())
                    
                    timestamp = idx / fps if fps > 0 else 0
                    
                    extracted_frames.append({
                        "frame_number": idx,
                        "timestamp_sec": round(timestamp, 2)
                    })
                    frames_data.append((frame_bytes, f"frame_{idx}.jpg"))
            
            cap.release()
            
            logger.info(f"Extracted {len(frames_data)} frames from video: {filename}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–¥—Ä—ã —á–µ—Ä–µ–∑ GigaChat Vision (multiple images)
            analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ {len(frames_data)} –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ.

{prompt}

–ö–∞–¥—Ä—ã –≤–∑—è—Ç—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ –≤–∏–¥–µ–æ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f} —Å–µ–∫).
–û–ø–∏—à–∏ —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏."""
            
            video_analysis = await gigachat_client.vision_analysis_multiple(
                files=frames_data,
                prompt=analysis_prompt,
                temperature=0.3
            )
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            audio_transcription = None
            if transcribe:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ ffmpeg
                    import subprocess
                    
                    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
                        temp_audio_path = temp_audio.name
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PCM WAV 16kHz mono
                    subprocess.run([
                        'ffmpeg', '-i', temp_video_path,
                        '-vn',  # –ë–µ–∑ –≤–∏–¥–µ–æ
                        '-acodec', 'pcm_s16le',  # PCM 16-bit
                        '-ar', '16000',  # 16kHz
                        '-ac', '1',  # –ú–æ–Ω–æ
                        temp_audio_path,
                        '-y'  # –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å
                    ], check=True, capture_output=True, stderr=subprocess.PIPE)
                    
                    # –ß–∏—Ç–∞–µ–º –∞—É–¥–∏–æ
                    with open(temp_audio_path, 'rb') as f:
                        audio_data = f.read()
                    
                    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
                    audio_transcription = await salutespeech_service.speech_to_text(
                        audio_data=audio_data,
                        sample_rate=16000,
                        bit_depth=16
                    )
                    
                    logger.info(f"Video audio transcribed: {len(audio_transcription)} chars")
                    
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ —Ñ–∞–π–ª
                    import os
                    os.unlink(temp_audio_path)
                    
                except Exception as e:
                    logger.warning(f"Failed to transcribe video audio: {e}")
                    audio_transcription = f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {str(e)}"
            
            result = {
                "analyzed_at": datetime.now(timezone.utc).isoformat(),
                "filename": filename,
                "prompt": prompt,
                "frame_count": len(extracted_frames),
                "video_duration_sec": round(duration, 2),
                "video_analysis": video_analysis,
                "audio_transcription": audio_transcription,
                "frames_analyzed": extracted_frames,
                "file_ref": file_ref or "auto_selected"
            }
            
            logger.info(f"Video analyzed: {filename}, {len(extracted_frames)} frames")
            return json.dumps(result, ensure_ascii=False, indent=2)
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤–∏–¥–µ–æ —Ñ–∞–π–ª
            import os
            os.unlink(temp_video_path)
        
    except Exception as e:
        logger.error(f"Failed to analyze video: {e}")
        return json.dumps({
            "error": str(e),
            "filename": filename if 'filename' in locals() else "unknown"
        }, ensure_ascii=False)


# ============================================================================
# MULTIMODAL ANALYSIS AGENT
# ============================================================================

class MultimodalAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ)
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (GigaChat Vision)
    - OCR —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    - –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ (SaluteSpeech STT)
    - –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ (–∫–∞–¥—Ä—ã + –∞—É–¥–∏–æ)
    """
    
    def __init__(self, minio_service: MinioService, llm=None):
        """
        Args:
            minio_service: –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏
            llm: LLM –¥–ª—è –∞–≥–µ–Ω—Ç–∞
        """
        from app.integrations.gigachat_client import GigaChatClient
        
        self.minio_service = minio_service or MinioServiceDep
        self.llm = llm or GigaChatClient().llm
        
        # –°–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self.tools = [
            analyze_image,
            ocr_image,
            transcribe_audio,
            analyze_video,
        ]
        
        logger.info("MultimodalAgent initialized with 4 tools")
    
    async def process(
        self,
        user_id: int,
        user_message: str,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        context = context or {}
        token = None
        minio_token = None
        
        try:
            tool_context = MultimodalContext(
                user_id=user_id,
                uploaded_files=context.get("uploaded_files", [])
            )
            
            ctx_token = _multimodal_context.set(tool_context)
            minio_token = _minio_service.set(self.minio_service) 
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
            files_info = ""
            if tool_context.uploaded_files:
                files_list = [
                    f"{f.get('filename', 'unknown')} ({f.get('file_type', 'unknown')})"
                    for f in tool_context.uploaded_files[:3]
                ]
                files_info = f"\nüìé –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {', '.join(files_list)}"
                if len(tool_context.uploaded_files) > 3:
                    files_info += f" –∏ –µ—â—ë {len(tool_context.uploaded_files) - 3}"
            
            # System prompt
            system_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É –¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –¥–æ–º–∞—à–Ω–∏—Ö –∂–∏–≤–æ—Ç–Ω—ã—Ö.

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ID: {user_id}{files_info}

**–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (4):**

1. **analyze_image** - –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ GigaChat Vision
   –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è: —Ñ–æ—Ç–æ –ø–∏—Ç–æ–º—Ü–∞, —Å–∏–º–ø—Ç–æ–º–æ–≤, —É—Å–ª–æ–≤–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥—ã
   
2. **ocr_image** - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (OCR)
   –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è: —ç—Ç–∏–∫–µ—Ç–æ–∫ –∫–æ—Ä–º–æ–≤, —Å–ø—Ä–∞–≤–æ–∫, —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–æ–≤, —Ä–µ—Ü–µ–ø—Ç–æ–≤
   –†–µ–∂–∏–º—ã: "plain" (–ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç) –∏–ª–∏ "structured" (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
   
3. **transcribe_audio** - –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç
   –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è: –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π, –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ
   
4. **analyze_video** - –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ (–∫–∞–¥—Ä—ã + –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∞—É–¥–∏–æ)
   –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è: –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–∏—Ç–æ–º—Ü–∞, —Å–∏–º–ø—Ç–æ–º–æ–≤ –≤ –¥–∏–Ω–∞–º–∏–∫–µ, –ø–æ—Ö–æ–¥–∫–∏
   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: frame_count (—Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤), transcribe (–∏–∑–≤–ª–µ—á—å –∞—É–¥–∏–æ)

**file_ref:**
- –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–µ—Ä—ë—Ç—Å—è –ø–µ—Ä–≤—ã–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
- –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —è–≤–Ω–æ object_name –∏–∑ MinIO

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç:**

–§–û–¢–û –ø–∏—Ç–æ–º—Ü–∞:
‚Üí analyze_image(prompt="–û—Ü–µ–Ω–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∏—Ç–æ–º—Ü–∞, –æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞...")

–≠–¢–ò–ö–ï–¢–ö–ê –∫–æ—Ä–º–∞:
‚Üí ocr_image(mode="structured") –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–∞

–°–ü–†–ê–í–ö–ê –æ—Ç –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∞:
‚Üí ocr_image(mode="structured") –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∏–∞–≥–Ω–æ–∑–∞, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

–ì–û–õ–û–°–û–í–û–ï —Å–æ–æ–±—â–µ–Ω–∏–µ:
‚Üí transcribe_audio()

–í–ò–î–ï–û —Å –ø–∏—Ç–æ–º—Ü–µ–º:
‚Üí analyze_video(prompt="–û–ø–∏—à–∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ, –æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞...", frame_count=5)

**–í–∞–∂–Ω–æ:**
- –í—Å–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç JSON –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
- –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ - —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –ü—Ä–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö - —Ä–µ–∫–æ–º–µ–Ω–¥—É–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∞
- –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ!"""
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])
            
            agent = create_tool_calling_agent(self.llm, self.tools, prompt)
            agent_executor = AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=settings.DEBUG,
                handle_parsing_errors=True,
                max_iterations=5,
            )
            
            result = await agent_executor.ainvoke({"input": user_message})
            return result.get("output", '{"error": "No output"}')
            
        except Exception as e:
            logger.exception(f"MultimodalAgent error for user {user_id}")
            return json.dumps({"error": str(e)}, ensure_ascii=False)
        finally:
            if ctx_token is not None:
                _multimodal_context.reset(ctx_token)
            if minio_token is not None:
                _minio_service.reset(minio_token)